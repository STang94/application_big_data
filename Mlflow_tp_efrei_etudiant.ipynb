{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP MLFLOW\n",
    "Florent Jakubowski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Prise en main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Installez le package mlflow avec python dans un environnement virtuel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Using cached mlflow-2.8.0-py3-none-any.whl (19.0 MB)\n",
      "Collecting alembic!=1.10.0,<2\n",
      "  Using cached alembic-1.12.1-py3-none-any.whl (226 kB)\n",
      "Collecting waitress<3\n",
      "  Using cached waitress-2.1.2-py3-none-any.whl (57 kB)\n",
      "Collecting sqlalchemy<3,>=1.4.0\n",
      "  Downloading SQLAlchemy-2.0.23-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/2.1 MB 544.7 kB/s eta 0:00:04\n",
      "     ------- -------------------------------- 0.4/2.1 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 1.1/2.1 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.9/2.1 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 7.3 MB/s eta 0:00:00\n",
      "Collecting Jinja2<4,>=3.0\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting pyarrow<14,>=4.0.0\n",
      "  Downloading pyarrow-13.0.0-cp39-cp39-win_amd64.whl (24.4 MB)\n",
      "     ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "     - -------------------------------------- 1.1/24.4 MB 35.0 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/24.4 MB 26.4 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 2.7/24.4 MB 21.7 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 3.7/24.4 MB 21.4 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 4.8/24.4 MB 22.0 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 6.2/24.4 MB 23.4 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 7.3/24.4 MB 23.4 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 8.8/24.4 MB 24.4 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 9.9/24.4 MB 24.4 MB/s eta 0:00:01\n",
      "     ----------------- --------------------- 11.2/24.4 MB 24.2 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 12.3/24.4 MB 24.2 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 13.7/24.4 MB 27.3 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 14.5/24.4 MB 27.3 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 15.7/24.4 MB 25.1 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 17.1/24.4 MB 26.2 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 18.5/24.4 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 19.7/24.4 MB 26.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 20.7/24.4 MB 25.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 22.0/24.4 MB 25.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 23.0/24.4 MB 25.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  24.4/24.4 MB 26.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  24.4/24.4 MB 26.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 24.4/24.4 MB 21.8 MB/s eta 0:00:00\n",
      "Collecting pandas<3\n",
      "  Downloading pandas-2.1.3-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "     ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 1.2/10.8 MB 24.9 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 2.5/10.8 MB 26.5 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 3.7/10.8 MB 26.6 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 4.9/10.8 MB 25.9 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 5.7/10.8 MB 24.4 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 7.1/10.8 MB 25.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 8.0/10.8 MB 24.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 9.3/10.8 MB 26.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 10.5/10.8 MB 26.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.8/10.8 MB 22.6 MB/s eta 0:00:00\n",
      "Collecting scipy<2\n",
      "  Downloading scipy-1.11.3-cp39-cp39-win_amd64.whl (44.3 MB)\n",
      "     ---------------------------------------- 0.0/44.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.3/44.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.3/44.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.3/44.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.3/44.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.3/44.3 MB 1.1 MB/s eta 0:00:39\n",
      "     ---------------------------------------- 0.6/44.3 MB 1.9 MB/s eta 0:00:23\n",
      "     - -------------------------------------- 1.5/44.3 MB 4.5 MB/s eta 0:00:10\n",
      "     -- ------------------------------------- 2.9/44.3 MB 7.8 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 3.7/44.3 MB 9.1 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 5.1/44.3 MB 10.8 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 6.2/44.3 MB 12.0 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 7.4/44.3 MB 13.2 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 8.7/44.3 MB 14.3 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 9.6/44.3 MB 14.7 MB/s eta 0:00:03\n",
      "     --------- ----------------------------- 11.0/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.2/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.2/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.2/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.2/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.2/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.2/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.2/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.2/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.2/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.2/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.2/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.2/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.2/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 11.2/44.3 MB 26.2 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 11.3/44.3 MB 9.6 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 11.4/44.3 MB 9.2 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 12.3/44.3 MB 9.0 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 13.6/44.3 MB 9.1 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 14.4/44.3 MB 8.8 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 15.6/44.3 MB 8.8 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 16.4/44.3 MB 8.8 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 17.1/44.3 MB 8.7 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 17.9/44.3 MB 8.6 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 19.2/44.3 MB 8.5 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 19.9/44.3 MB 8.5 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 21.0/44.3 MB 8.4 MB/s eta 0:00:03\n",
      "     ------------------- ------------------- 21.8/44.3 MB 21.1 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 22.9/44.3 MB 21.1 MB/s eta 0:00:02\n",
      "     --------------------- ----------------- 24.1/44.3 MB 21.1 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 25.2/44.3 MB 20.5 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 26.0/44.3 MB 19.8 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 27.0/44.3 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 28.1/44.3 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 29.2/44.3 MB 21.1 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 30.5/44.3 MB 21.9 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 31.3/44.3 MB 21.9 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 32.2/44.3 MB 21.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 32.3/44.3 MB 5.1 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 32.3/44.3 MB 5.0 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 32.4/44.3 MB 4.8 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 32.6/44.3 MB 4.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 32.9/44.3 MB 4.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 33.1/44.3 MB 4.6 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 33.4/44.3 MB 4.5 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 33.7/44.3 MB 4.5 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 34.2/44.3 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 34.6/44.3 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 35.1/44.3 MB 4.3 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 35.8/44.3 MB 4.3 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 36.7/44.3 MB 4.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 37.8/44.3 MB 4.3 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 38.9/44.3 MB 4.3 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 40.0/44.3 MB 4.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 41.3/44.3 MB 4.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 42.7/44.3 MB 14.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  43.8/44.3 MB 19.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.3/44.3 MB 20.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.3/44.3 MB 20.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.3/44.3 MB 20.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 44.3/44.3 MB 14.5 MB/s eta 0:00:00\n",
      "Collecting pyyaml<7,>=5.1\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "     ---------------------------------------- 0.0/152.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 152.8/152.8 kB 8.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from mlflow) (6.8.0)\n",
      "Collecting Flask<4\n",
      "  Using cached flask-3.0.0-py3-none-any.whl (99 kB)\n",
      "Collecting cloudpickle<3\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: packaging<24 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from mlflow) (23.2)\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Using cached sqlparse-0.4.4-py3-none-any.whl (41 kB)\n",
      "Collecting querystring-parser<2\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting matplotlib<4\n",
      "  Using cached matplotlib-3.8.1-cp39-cp39-win_amd64.whl (7.6 MB)\n",
      "Collecting entrypoints<1\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting protobuf<5,>=3.12.0\n",
      "  Using cached protobuf-4.25.0-cp39-cp39-win_amd64.whl (413 kB)\n",
      "Collecting databricks-cli<1,>=0.8.7\n",
      "  Using cached databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
      "Requirement already satisfied: psutil<6 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from mlflow) (5.9.6)\n",
      "Collecting scikit-learn<2\n",
      "  Downloading scikit_learn-1.3.2-cp39-cp39-win_amd64.whl (9.3 MB)\n",
      "     ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 1.2/9.3 MB 25.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 2.5/9.3 MB 32.0 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 3.7/9.3 MB 26.3 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 5.3/9.3 MB 28.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 6.3/9.3 MB 26.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 7.7/9.3 MB 27.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 8.7/9.3 MB 26.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  9.3/9.3 MB 27.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 9.3/9.3 MB 23.0 MB/s eta 0:00:00\n",
      "Collecting click<9,>=7.0\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting pytz<2024\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Collecting docker<7,>=4.0.0\n",
      "  Using cached docker-6.1.3-py3-none-any.whl (148 kB)\n",
      "Collecting markdown<4,>=3.3\n",
      "  Using cached Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "Collecting gitpython<4,>=2.1.0\n",
      "  Using cached GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "Collecting numpy<2\n",
      "  Downloading numpy-1.26.2-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "     ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 1.2/15.8 MB 24.8 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 2.3/15.8 MB 28.8 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 3.5/15.8 MB 27.6 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 4.7/15.8 MB 24.9 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 5.5/15.8 MB 23.4 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 6.7/15.8 MB 23.7 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 7.8/15.8 MB 25.1 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 8.8/15.8 MB 24.5 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 9.8/15.8 MB 24.2 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 10.5/15.8 MB 22.6 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 11.3/15.8 MB 21.8 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 12.0/15.8 MB 21.8 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 12.6/15.8 MB 20.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 13.1/15.8 MB 19.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 13.8/15.8 MB 18.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 14.4/15.8 MB 17.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 15.2/15.8 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.4/15.8 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.4/15.8 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.4/15.8 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.4/15.8 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.4/15.8 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.4/15.8 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.5/15.8 MB 11.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.5/15.8 MB 11.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.8/15.8 MB 11.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 15.8/15.8 MB 10.2 MB/s eta 0:00:00\n",
      "Collecting requests<3,>=2.17.3\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (4.8.0)\n",
      "Collecting Mako\n",
      "  Using cached Mako-1.3.0-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: colorama in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from click<9,>=7.0->mlflow) (0.4.6)\n",
      "Collecting urllib3<3,>=1.26.7\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Collecting pyjwt>=1.7.0\n",
      "  Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Collecting oauthlib>=3.1.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Using cached websocket_client-1.6.4-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from docker<7,>=4.0.0->mlflow) (306)\n",
      "Collecting blinker>=1.6.2\n",
      "  Using cached blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting itsdangerous>=2.1.2\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting Werkzeug>=3.0.0\n",
      "  Using cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.17.0)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.3-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.44.1-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     -------- ------------------------------- 0.5/2.1 MB 14.9 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.8/2.1 MB 9.8 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.2/2.1 MB 9.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 1.8/2.1 MB 10.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 10.5 MB/s eta 0:00:00\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Using cached kiwisolver-1.4.5-cp39-cp39-win_amd64.whl (56 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from matplotlib<4->mlflow) (2.8.2)\n",
      "Collecting pillow>=8\n",
      "  Using cached Pillow-10.1.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Using cached importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.2.0-cp39-cp39-win_amd64.whl (181 kB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl (100 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.0.1-cp39-cp39-win_amd64.whl (287 kB)\n",
      "     ---------------------------------------- 0.0/287.1 kB ? eta -:--:--\n",
      "     ------------------------------------- 287.1/287.1 kB 17.3 MB/s eta 0:00:00\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pytz, websocket-client, waitress, urllib3, tzdata, threadpoolctl, tabulate, sqlparse, smmap, querystring-parser, pyyaml, pyparsing, pyjwt, protobuf, pillow, oauthlib, numpy, MarkupSafe, kiwisolver, joblib, itsdangerous, importlib-resources, idna, greenlet, fonttools, entrypoints, cycler, cloudpickle, click, charset-normalizer, certifi, blinker, Werkzeug, sqlalchemy, scipy, requests, pyarrow, pandas, markdown, Mako, Jinja2, gitdb, contourpy, scikit-learn, matplotlib, gitpython, Flask, docker, databricks-cli, alembic, mlflow\n",
      "Successfully installed Flask-3.0.0 Jinja2-3.1.2 Mako-1.3.0 MarkupSafe-2.1.3 Werkzeug-3.0.1 alembic-1.12.1 blinker-1.7.0 certifi-2023.7.22 charset-normalizer-3.3.2 click-8.1.7 cloudpickle-2.2.1 contourpy-1.2.0 cycler-0.12.1 databricks-cli-0.18.0 docker-6.1.3 entrypoints-0.4 fonttools-4.44.1 gitdb-4.0.11 gitpython-3.1.40 greenlet-3.0.1 idna-3.4 importlib-resources-6.1.1 itsdangerous-2.1.2 joblib-1.3.2 kiwisolver-1.4.5 markdown-3.5.1 matplotlib-3.8.1 mlflow-2.8.0 numpy-1.26.2 oauthlib-3.2.2 pandas-2.1.3 pillow-10.1.0 protobuf-4.25.0 pyarrow-13.0.0 pyjwt-2.8.0 pyparsing-3.1.1 pytz-2023.3.post1 pyyaml-6.0.1 querystring-parser-1.2.4 requests-2.31.0 scikit-learn-1.3.2 scipy-1.11.3 smmap-5.0.1 sqlalchemy-2.0.23 sqlparse-0.4.4 tabulate-0.9.0 threadpoolctl-3.2.0 tzdata-2023.3 urllib3-2.1.0 waitress-2.1.2 websocket-client-1.6.4\n"
     ]
    }
   ],
   "source": [
    "#pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Dans un terminal lancez un serveur mlflow. Aller voir dans votre navigateur, sur le port correspondant, l'ui de mlflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque vous lancez mflow sans option par défaut mlflow va stocker toute la donnée dont il a besoin sur votre file system. Vous aurez notamment un dossier mlruns qui se créra par défaut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Dans un notebook, utilisez le package mlflow pour vous connecter au serveur mlflow que vous avez lancé. Utilisez la bonne fonction pour paramétrer l'adresse du serveur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\pydantic\\_internal\\_fields.py:149: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\pydantic\\_internal\\_config.py:318: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set the MLflow tracking URI to the address of your server\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "# Example: Track a metric and parameter\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"param1\", 5)\n",
    "    mlflow.log_metric(\"metric1\", 10.2)\n",
    "\n",
    "# To stop tracking in this run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention ! A chaque fois que vous effectuerez une opération sur mlflow dans une autre cellule de votre notebook vous devrez vérifier avant que vous pointez bien sur le bon serveur mlflow. Il existe aussi une fonction pour connaître quelle adresse de serveur a été enregistrée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adresse actuelle du serveur MLflow : http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Obtenir l'adresse actuelle du serveur MLflow\n",
    "current_tracking_uri = mlflow.get_tracking_uri()\n",
    "print(f\"Adresse actuelle du serveur MLflow : {current_tracking_uri}\")\n",
    "\n",
    "# On peut changer l'adresse du serveur avec cette commande\n",
    "# mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créez une experiment via votre notebook ou avec via l'ui. \n",
    "Une experiment, ou une expérience en français, est un ensemble de run que vous avez effectué. Le but est de trouver in fine les meilleurs paramètres, modèles ou hyperparamètres pour votre besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MLflow tracking URI to the address of your server\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "id = mlflow.create_experiment(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pareillement à l'adresse du serveur mlflow à chaque fois que vous exécuterez un run dans une cellule vous devrez définir l'experiment sur laquelle vous voulez envoyer votre run. Cherchez dans la documentation la fonction permettant de faire cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Track a metric and parameter\n",
    "with mlflow.start_run(experiment_id=id):\n",
    "    mlflow.log_param(\"param1\", 5)\n",
    "    mlflow.log_metric(\"metric1\", 10.2)\n",
    "    \n",
    "\n",
    "# To stop tracking in this run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = mlflow.get_experiment_by_name(\"tet\")\n",
    "type(test)\n",
    "test==None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous vous proposons de créer une fonction `configure_experiment` permettant de créer une expérience ou de définir l'expérience si elle existe déjà."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    # Commencer un nouveau run dans l'expérience existante\\n    with mlflow.start_run(experiment_id=experiment_id):\\n        for key,value in params:\\n             mlflow.log_param(key, value)\\n        for key,value in metrics:\\n            mlflow.log_metric(key, value)\\n        for key,value in tags:\\n            mlflow.set_tag(key, value)\\n        for path in artifacts:\\n            mlflow.log_artifact(path)\\n        # To stop tracking in this run\\n    mlflow.end_run()\\n\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def configure_experiment(name:str, params:dict={\"\",\"\"},metrics:dict={\"\",\"\"},tags:dict={\"\",\"\"}, artifacts:list=[\"\"]):\n",
    "\n",
    "    # track du serveur\n",
    "    mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "    if mlflow.get_experiment_by_name(name) is None:\n",
    "        \n",
    "        return mlflow.create_experiment(name)\n",
    "        \n",
    "    else :\n",
    "        # Récupérer l'ID de l'expérience\n",
    "        return mlflow.get_experiment_by_name(name).experiment_id\n",
    "\"\"\"\n",
    "    # Commencer un nouveau run dans l'expérience existante\n",
    "    with mlflow.start_run(experiment_id=experiment_id):\n",
    "        for key,value in params:\n",
    "             mlflow.log_param(key, value)\n",
    "        for key,value in metrics:\n",
    "            mlflow.log_metric(key, value)\n",
    "        for key,value in tags:\n",
    "            mlflow.set_tag(key, value)\n",
    "        for path in artifacts:\n",
    "            mlflow.log_artifact(path)\n",
    "        # To stop tracking in this run\n",
    "    mlflow.end_run()\n",
    "\"\"\"\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Après avoir créé l'experiment. Nous allons entraîner notre modèle. Prenez le dataset wine de la librairie sklearn et utilisez un algorithme de la famille des arbres de décisions. Nous allons lors de l'entraînement de notre modèle logger les mesures.   \n",
    "\n",
    "Pour cela nous voulons lancer un nouveau run dans notre experiment sur mlflow. Le run correspond à un entraînement du modèle.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Créer une variable run_name pour le nouveau run que vous voulez créer avec la date du jour, l'heure, la minute et la seconde dans le nom.   \n",
    "-Trouvez comment logger les paramètres d'entraînement, les hyperparamètres et les performances du modèle (metrics) explicitement dans mlflow en lançant votre run de manière manuelle.   \n",
    "-Ajoutez également votre modèle avec un nom distinctif grâce à la méthode adéquat, vous devrez réutiliser ce nom lors des prochains entraînements. Que voyez-vous dans l'ui de mlflow ?   \n",
    "\n",
    "\n",
    "Vous pouvez utiliser `with` pour ne pas avoir besoin d'utiliser la fonction `end_run()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset Wine\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser le dataset en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paramètres de l'algorithme de l'arbre de décision\n",
    "tree_params = {\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': 3,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# track du serveur\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "        \n",
    "experiment_id = configure_experiment(\"wine_classification_experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'183478325808655477'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: None\n",
      "<ActiveRun: >\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(experiment_id=experiment_id,run_name=f\"run_{datetime.datetime.now().strftime('%Y/%m/%d_%H:%M:%S')}\"):\n",
    "    for key, value in tree_params.items():\n",
    "        mlflow.log_param(key, value)\n",
    "        \n",
    "    # Initialiser et entraîner le modèle\n",
    "    model = DecisionTreeClassifier(**tree_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "     # Calcul des métriques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Log des métriques\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Log des artefacts (le modèle)\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    # Ajouter le modèle avec un nom distinctif\n",
    "    model_name = \"decision_tree_model\"\n",
    "    mlflow.sklearn.log_model(model, model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut utiliser `mlflow.active_run()` pour être sûr que le run est bien terminé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Afficher le résultat du run (ID du run)\n",
    "print(mlflow.active_run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecrivez un run sans utiliser un with et sans utilisez mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: <ActiveRun: >\n"
     ]
    }
   ],
   "source": [
    "mlflow.start_run(experiment_id=experiment_id,run_name=f\"run_{datetime.datetime.now().strftime('%Y/%m/%d_%H:%M:%S')}\")\n",
    "for key, value in tree_params.items():\n",
    "        mlflow.log_param(key, value)\n",
    "        \n",
    "    # Initialiser et entraîner le modèle\n",
    "model = DecisionTreeClassifier(**tree_params)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "    # Calcul des métriques\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "    # Log des métriques\n",
    "mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Log des artefacts (le modèle)\n",
    "mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    # Ajouter le modèle avec un nom distinctif\n",
    "model_name = \"decision_tree_model\"\n",
    "mlflow.sklearn.log_model(model, model_name)\n",
    "\n",
    "# Afficher le résultat du run (ID du run)\n",
    "print(f\"Run ID: {mlflow.active_run()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `active_run()` nous retourne normalement ce run ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ActiveRun: >\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.active_run()\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faisons bien attention de bien le fermer pour ne pas avoir de comportements exotiques ensuite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez mlflow.end_run() et vérifiez avec active_run() qu'aucun run n'est retourné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()\n",
    "print(mlflow.active_run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe une autre manière de logger automatiquement des paramètres et des metrics à vous de la trouver.   \n",
    "(Attention une fois activée cette fonction entrainera toujours un log automatique, veillez à la désactiver pour la suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.sklearn import autolog\n",
    "# Activer le suivi automatique pour scikit-learn\n",
    "autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Changez un ou plusieurs hyperparamètres et relancez un entraînement avec le même nom de modèle. Rendez-vous dans l'onglet model de mlflow que constatez-vous ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.sklearn import autolog\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Charger le dataset Wine\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Diviser le dataset en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Paramètres de l'algorithme de l'arbre de décision\n",
    "tree_params = {\n",
    "    'criterion': 'entropy',  # Changer un hyperparamètre\n",
    "    'max_depth': 5,           # Changer un autre hyperparamètre\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Configuration de l'expérience dans MLflow\n",
    "experiment_name = \"wine_classification_experiment\"\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "experiment_id = configure_experiment(experiment_name)\n",
    "\n",
    "# Commencer un nouveau run avec le même nom de modèle\n",
    "with mlflow.start_run(experiment_id=experiment_id,run_name=f\"run_{datetime.datetime.now().strftime('%Y/%m/%d_%H:%M:%S')}\"):\n",
    "    for key, value in tree_params.items():\n",
    "        mlflow.log_param(key, value)\n",
    "        \n",
    "    # Initialiser et entraîner le modèle\n",
    "    model = DecisionTreeClassifier(**tree_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "     # Calcul des métriques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Log des métriques\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Log des artefacts (le modèle)\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    # Ajouter le modèle avec un nom distinctif\n",
    "    model_name = \"decision_tree_model\"\n",
    "    mlflow.sklearn.log_model(model, model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez récupérer le dernier run avec la fonction last_active_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Run: data=<RunData: metrics={'accuracy': 0.9166666666666666}, params={'criterion': 'entropy', 'max_depth': '5', 'random_state': '42'}, tags={'mlflow.log-model.history': '[{\"run_id\": \"aae9d3a3ee0a4976a448a66a240c29d0\", '\n",
      "                             '\"artifact_path\": \"model\", \"utc_time_created\": '\n",
      "                             '\"2023-11-15 10:54:41.334816\", \"flavors\": '\n",
      "                             '{\"python_function\": {\"model_path\": \"model.pkl\", '\n",
      "                             '\"predict_fn\": \"predict\", \"loader_module\": '\n",
      "                             '\"mlflow.sklearn\", \"python_version\": \"3.9.18\", '\n",
      "                             '\"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": '\n",
      "                             '\"python_env.yaml\"}}, \"sklearn\": '\n",
      "                             '{\"pickled_model\": \"model.pkl\", '\n",
      "                             '\"sklearn_version\": \"1.3.2\", '\n",
      "                             '\"serialization_format\": \"cloudpickle\", \"code\": '\n",
      "                             'null}}, \"model_uuid\": '\n",
      "                             '\"b35ab12d768644898c1da28a04ef5777\", '\n",
      "                             '\"mlflow_version\": \"2.8.0\", \"model_size_bytes\": '\n",
      "                             '2270}, {\"run_id\": '\n",
      "                             '\"aae9d3a3ee0a4976a448a66a240c29d0\", '\n",
      "                             '\"artifact_path\": \"decision_tree_model\", '\n",
      "                             '\"utc_time_created\": \"2023-11-15 '\n",
      "                             '10:54:44.943528\", \"flavors\": {\"python_function\": '\n",
      "                             '{\"model_path\": \"model.pkl\", \"predict_fn\": '\n",
      "                             '\"predict\", \"loader_module\": \"mlflow.sklearn\", '\n",
      "                             '\"python_version\": \"3.9.18\", \"env\": {\"conda\": '\n",
      "                             '\"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, '\n",
      "                             '\"sklearn\": {\"pickled_model\": \"model.pkl\", '\n",
      "                             '\"sklearn_version\": \"1.3.2\", '\n",
      "                             '\"serialization_format\": \"cloudpickle\", \"code\": '\n",
      "                             'null}}, \"model_uuid\": '\n",
      "                             '\"86b3acfa0dfc4318a1b04ecc20e88d36\", '\n",
      "                             '\"mlflow_version\": \"2.8.0\", \"model_size_bytes\": '\n",
      "                             '2270}]',\n",
      " 'mlflow.runName': 'run_2023/11/15_11:54:41',\n",
      " 'mlflow.source.name': \"c:\\\\Ecole d'inge\\\\Application of big \"\n",
      "                       'data\\\\venv\\\\lib\\\\site-packages\\\\ipykernel_launcher.py',\n",
      " 'mlflow.source.type': 'LOCAL',\n",
      " 'mlflow.user': 'tangs'}>, info=<RunInfo: artifact_uri='mlflow-artifacts:/183478325808655477/aae9d3a3ee0a4976a448a66a240c29d0/artifacts', end_time=1700045687963, experiment_id='183478325808655477', lifecycle_stage='active', run_id='aae9d3a3ee0a4976a448a66a240c29d0', run_name='run_2023/11/15_11:54:41', run_uuid='aae9d3a3ee0a4976a448a66a240c29d0', start_time=1700045681223, status='FINISHED', user_id='tangs'>, inputs=<RunInputs: dataset_inputs=[]>>\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.last_active_run() \n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Entraîner 3,4 modèles avec des hyperparamètres différents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.sklearn import autolog\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Charger le dataset Wine\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Diviser le dataset en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Paramètres de l'algorithme de l'arbre de décision\n",
    "tree_params = {\n",
    "    'criterion': 'log_loss',  # Changer un hyperparamètre\n",
    "    'max_depth': 9,           # Changer un autre hyperparamètre\n",
    "    'random_state': 36\n",
    "}\n",
    "\n",
    "# Configuration de l'expérience dans MLflow\n",
    "experiment_name = \"wine_classification_experiment\"\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "experiment_id = configure_experiment(experiment_name)\n",
    "\n",
    "# Commencer un nouveau run avec le même nom de modèle\n",
    "with mlflow.start_run(experiment_id=experiment_id,run_name=f\"run_{datetime.datetime.now().strftime('%Y/%m/%d_%H:%M:%S')}\"):\n",
    "    for key, value in tree_params.items():\n",
    "        mlflow.log_param(key, value)\n",
    "        \n",
    "    # Initialiser et entraîner le modèle\n",
    "    model = DecisionTreeClassifier(**tree_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "     # Calcul des métriques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Log des métriques\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Log des artefacts (le modèle)\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    # Ajouter le modèle avec un nom distinctif\n",
    "    model_name = \"decision_tree_model\"\n",
    "    mlflow.sklearn.log_model(model, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.sklearn import autolog\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Charger le dataset Wine\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Diviser le dataset en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Paramètres de l'algorithme de l'arbre de décision\n",
    "tree_params = {\n",
    "    'criterion': 'gini',  # Changer un hyperparamètre\n",
    "    'max_depth': 13,           # Changer un autre hyperparamètre\n",
    "    'random_state': 50\n",
    "}\n",
    "\n",
    "# Configuration de l'expérience dans MLflow\n",
    "experiment_name = \"wine_classification_experiment\"\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "experiment_id = configure_experiment(experiment_name)\n",
    "\n",
    "# Commencer un nouveau run avec le même nom de modèle\n",
    "with mlflow.start_run(experiment_id=experiment_id,run_name=f\"run_{datetime.datetime.now().strftime('%Y/%m/%d_%H:%M:%S')}\"):\n",
    "    for key, value in tree_params.items():\n",
    "        mlflow.log_param(key, value)\n",
    "        \n",
    "    # Initialiser et entraîner le modèle\n",
    "    model = DecisionTreeClassifier(**tree_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "     # Calcul des métriques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Log des métriques\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Log des artefacts (le modèle)\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    # Ajouter le modèle avec un nom distinctif\n",
    "    model_name = \"decision_tree_model\"\n",
    "    mlflow.sklearn.log_model(model, model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Versionning des données avec DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque run le dataset utilisé est précisé dans mlflow. Mais les informations fournies sont assez pauvres. Pour améliorer ça nous allons utiliser l'outil dvc en combinaison avec mlflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dvc fonctionne de pair avec git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisez un repo git localement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in C:/Ecole d'inge/Application of big data/.git/\n"
     ]
    }
   ],
   "source": [
    "!git init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installez dvc avec pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dvc\n",
      "  Downloading dvc-3.29.0-py3-none-any.whl (429 kB)\n",
      "     ---------------------------------------- 0.0/429.9 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/429.9 kB ? eta -:--:--\n",
      "     ---------- --------------------------- 122.9/429.9 kB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 429.9/429.9 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama>=0.3.9 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from dvc) (0.4.6)\n",
      "Collecting flufl.lock<8,>=5\n",
      "  Downloading flufl.lock-7.1.1-py3-none-any.whl (11 kB)\n",
      "Collecting shortuuid>=0.5\n",
      "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Collecting flatten-dict<1,>=0.4.1\n",
      "  Downloading flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting dvc-http>=2.29.0\n",
      "  Downloading dvc_http-2.30.2-py3-none-any.whl (12 kB)\n",
      "Collecting iterative-telemetry>=0.0.7\n",
      "  Downloading iterative_telemetry-0.0.8-py3-none-any.whl (10 kB)\n",
      "Collecting scmrepo<2,>=1.4.1\n",
      "  Downloading scmrepo-1.4.1-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.0/58.0 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting grandalf<1,>=0.7\n",
      "  Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n",
      "     ---------------------------------------- 0.0/41.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.8/41.8 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from dvc) (3.1.1)\n",
      "Collecting funcy>=1.14\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.7 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from dvc) (0.9.0)\n",
      "Collecting configobj>=5.0.6\n",
      "  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
      "Collecting shtab<2,>=1.3.4\n",
      "  Downloading shtab-1.6.4-py3-none-any.whl (13 kB)\n",
      "Collecting hydra-core>=1.1\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "     ---------------------------------------- 0.0/154.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 154.5/154.5 kB ? eta 0:00:00\n",
      "Collecting tqdm<5,>=4.63.1\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.3/78.3 kB 4.3 MB/s eta 0:00:00\n",
      "Collecting zc.lockfile>=1.2.1\n",
      "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
      "Collecting dvc-data<2.21.0,>=2.20.0\n",
      "  Downloading dvc_data-2.20.0-py3-none-any.whl (68 kB)\n",
      "     ---------------------------------------- 0.0/68.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 68.0/68.0 kB ? eta 0:00:00\n",
      "Collecting platformdirs<4,>=3.1.1\n",
      "  Using cached platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
      "Collecting tomlkit>=0.11.1\n",
      "  Downloading tomlkit-0.12.3-py3-none-any.whl (37 kB)\n",
      "Collecting dvc-task<1,>=0.3.0\n",
      "  Downloading dvc_task-0.3.0-py3-none-any.whl (21 kB)\n",
      "Collecting pydot>=1.2.4\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting voluptuous>=0.11.7\n",
      "  Downloading voluptuous-0.14.0-py3-none-any.whl (30 kB)\n",
      "Collecting networkx>=2.5\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     ---------------- ----------------------- 0.7/1.6 MB 14.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.3/1.6 MB 13.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.6/1.6 MB 13.1 MB/s eta 0:00:00\n",
      "Collecting rich>=12\n",
      "  Downloading rich-13.6.0-py3-none-any.whl (239 kB)\n",
      "     ---------------------------------------- 0.0/239.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 239.8/239.8 kB 15.3 MB/s eta 0:00:00\n",
      "Collecting dvc-studio-client<1,>=0.13.0\n",
      "  Downloading dvc_studio_client-0.15.0-py3-none-any.whl (13 kB)\n",
      "Collecting ruamel.yaml>=0.17.11\n",
      "  Downloading ruamel.yaml-0.18.5-py3-none-any.whl (116 kB)\n",
      "     ---------------------------------------- 0.0/116.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 116.4/116.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.22 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from dvc) (2.31.0)\n",
      "Collecting pathspec>=0.10.3\n",
      "  Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: packaging>=19 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from dvc) (23.2)\n",
      "Requirement already satisfied: psutil>=5.8 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from dvc) (5.9.6)\n",
      "Collecting gto<2,>=1.4.0\n",
      "  Downloading gto-1.5.0-py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 0.0/46.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.6/46.6 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting distro>=1.3\n",
      "  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Collecting dpath<3,>=2.1.0\n",
      "  Downloading dpath-2.1.6-py3-none-any.whl (17 kB)\n",
      "Collecting dvc-render<1,>=0.3.1\n",
      "  Downloading dvc_render-0.6.0-py3-none-any.whl (19 kB)\n",
      "Collecting pygtrie>=2.3.2\n",
      "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: six in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from configobj>=5.0.6->dvc) (1.16.0)\n",
      "Collecting dictdiffer>=0.8.1\n",
      "  Downloading dictdiffer-0.9.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting diskcache>=5.2.1\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.5/45.5 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting attrs>=21.3.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting sqltrie<1,>=0.8.0\n",
      "  Downloading sqltrie-0.8.0-py3-none-any.whl (17 kB)\n",
      "Collecting dvc-objects<2,>=1.1.0\n",
      "  Downloading dvc_objects-1.2.0-py3-none-any.whl (38 kB)\n",
      "Collecting aiohttp-retry>=2.5.0\n",
      "  Downloading aiohttp_retry-2.8.3-py3-none-any.whl (9.8 kB)\n",
      "Collecting fsspec[http]\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "     ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 166.4/166.4 kB 9.8 MB/s eta 0:00:00\n",
      "Collecting dulwich\n",
      "  Downloading dulwich-0.21.6-cp39-cp39-win_amd64.whl (484 kB)\n",
      "     ---------------------------------------- 0.0/484.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 484.9/484.9 kB 14.8 MB/s eta 0:00:00\n",
      "Collecting celery<6,>=5.3.0\n",
      "  Downloading celery-5.3.5-py3-none-any.whl (421 kB)\n",
      "     ---------------------------------------- 0.0/421.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 421.9/421.9 kB 27.4 MB/s eta 0:00:00\n",
      "Collecting kombu<6,>=5.3.0\n",
      "  Downloading kombu-5.3.3-py3-none-any.whl (199 kB)\n",
      "     ---------------------------------------- 0.0/199.1 kB ? eta -:--:--\n",
      "     ------------------------------------- 199.1/199.1 kB 12.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pywin32>=225 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from dvc-task<1,>=0.3.0->dvc) (306)\n",
      "Collecting atpublic>=2.3\n",
      "  Downloading atpublic-4.0-py3-none-any.whl (4.9 kB)\n",
      "Collecting typer>=0.4.1\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.9/45.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: entrypoints in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from gto<2,>=1.4.0->dvc) (0.4)\n",
      "Collecting semver>=3.0.0\n",
      "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Collecting pydantic!=2.0.0,<3,>=1.9.0\n",
      "  Downloading pydantic-2.5.0-py3-none-any.whl (407 kB)\n",
      "     ---------------------------------------- 0.0/407.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 407.5/407.5 kB 12.4 MB/s eta 0:00:00\n",
      "Collecting antlr4-python3-runtime==4.9.*\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "     ---------------------------------------- 0.0/117.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 117.0/117.0 kB 6.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting omegaconf<2.4,>=2.2\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 0.0/79.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 79.5/79.5 kB ? eta 0:00:00\n",
      "Collecting appdirs\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from requests>=2.22->dvc) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from requests>=2.22->dvc) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from requests>=2.22->dvc) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from requests>=2.22->dvc) (3.4)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "     ---------------------------------------- 0.0/87.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 87.5/87.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from rich>=12->dvc) (2.16.1)\n",
      "Collecting ruamel.yaml.clib>=0.2.7\n",
      "  Downloading ruamel.yaml.clib-0.2.8-cp39-cp39-win_amd64.whl (118 kB)\n",
      "     ---------------------------------------- 0.0/118.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 118.4/118.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: gitpython>3 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from scmrepo<2,>=1.4.1->dvc) (3.1.40)\n",
      "Collecting asyncssh<3,>=2.13.1\n",
      "  Downloading asyncssh-2.14.1-py3-none-any.whl (352 kB)\n",
      "     ---------------------------------------- 0.0/352.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 352.2/352.2 kB 11.0 MB/s eta 0:00:00\n",
      "Collecting pygit2>=1.13.0\n",
      "  Downloading pygit2-1.13.2-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     --------------------------------- ------ 1.0/1.2 MB 33.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.0/1.2 MB 33.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.0/1.2 MB 33.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.0/1.2 MB 33.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.2/1.2 MB 6.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from zc.lockfile>=1.2.1->dvc) (58.1.0)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.6-cp39-cp39-win_amd64.whl (329 kB)\n",
      "     ---------------------------------------- 0.0/329.0 kB ? eta -:--:--\n",
      "     ------------------------------------- 329.0/329.0 kB 21.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from asyncssh<3,>=2.13.1->scmrepo<2,>=1.4.1->dvc) (4.8.0)\n",
      "Collecting cryptography>=39.0\n",
      "  Using cached cryptography-41.0.5-cp37-abi3-win_amd64.whl (2.7 MB)\n",
      "Collecting billiard<5.0,>=4.2.0\n",
      "  Downloading billiard-4.2.0-py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 0.0/86.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 86.7/86.7 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click<9.0,>=8.1.2 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (8.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (2.8.2)\n",
      "Collecting click-repl>=0.2.0\n",
      "  Downloading click_repl-0.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting click-plugins>=1.1.1\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting vine<6.0,>=5.1.0\n",
      "  Downloading vine-5.1.0-py3-none-any.whl (9.6 kB)\n",
      "Collecting click-didyoumean>=0.3.0\n",
      "  Downloading click_didyoumean-0.3.0-py3-none-any.whl (2.7 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (2023.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from gitpython>3->scmrepo<2,>=1.4.1->dvc) (4.0.11)\n",
      "Collecting amqp<6.0.0,>=5.1.1\n",
      "  Downloading amqp-5.2.0-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.9/50.9 kB ? eta 0:00:00\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.1->dvc) (6.0.1)\n",
      "Collecting pydantic-core==2.14.1\n",
      "  Downloading pydantic_core-2.14.1-cp39-none-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "     ------------------- -------------------- 0.9/1.9 MB 30.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 24.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.9/1.9 MB 20.0 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting cffi>=1.16.0\n",
      "  Downloading cffi-1.16.0-cp39-cp39-win_amd64.whl (181 kB)\n",
      "     ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 181.6/181.6 kB ? eta 0:00:00\n",
      "Collecting orjson\n",
      "  Downloading orjson-3.9.10-cp39-none-win_amd64.whl (134 kB)\n",
      "     ---------------------------------------- 0.0/134.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 134.9/134.9 kB ? eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.0-cp39-cp39-win_amd64.whl (44 kB)\n",
      "     ---------------------------------------- 0.0/44.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.7/44.7 kB ? eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp39-cp39-win_amd64.whl (61 kB)\n",
      "     ---------------------------------------- 0.0/61.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.7/61.7 kB ? eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.36 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from click-repl>=0.2.0->celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (3.0.41)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython>3->scmrepo<2,>=1.4.1->dvc) (5.0.1)\n",
      "Requirement already satisfied: wcwidth in c:\\ecole d'inge\\application of big data\\venv\\lib\\site-packages (from prompt-toolkit>=3.0.36->click-repl>=0.2.0->celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (0.2.10)\n",
      "Installing collected packages: voluptuous, pygtrie, funcy, dictdiffer, appdirs, antlr4-python3-runtime, zc.lockfile, vine, tqdm, tomlkit, shtab, shortuuid, semver, ruamel.yaml.clib, pydot, pydantic-core, pycparser, platformdirs, pathspec, orjson, omegaconf, networkx, multidict, mdurl, grandalf, fsspec, frozenlist, flatten-dict, filelock, dvc-render, dulwich, dpath, distro, diskcache, configobj, billiard, attrs, atpublic, async-timeout, annotated-types, yarl, typer, sqltrie, ruamel.yaml, pydantic, markdown-it-py, iterative-telemetry, hydra-core, flufl.lock, dvc-studio-client, dvc-objects, click-repl, click-plugins, click-didyoumean, cffi, amqp, aiosignal, rich, pygit2, kombu, dvc-data, cryptography, aiohttp, celery, asyncssh, aiohttp-retry, scmrepo, dvc-task, dvc-http, gto, dvc\n",
      "  Running setup.py install for antlr4-python3-runtime: started\n",
      "  Running setup.py install for antlr4-python3-runtime: finished with status 'done'\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 4.0.0\n",
      "    Uninstalling platformdirs-4.0.0:\n",
      "      Successfully uninstalled platformdirs-4.0.0\n",
      "Successfully installed aiohttp-3.8.6 aiohttp-retry-2.8.3 aiosignal-1.3.1 amqp-5.2.0 annotated-types-0.6.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 async-timeout-4.0.3 asyncssh-2.14.1 atpublic-4.0 attrs-23.1.0 billiard-4.2.0 celery-5.3.5 cffi-1.16.0 click-didyoumean-0.3.0 click-plugins-1.1.1 click-repl-0.3.0 configobj-5.0.8 cryptography-41.0.5 dictdiffer-0.9.0 diskcache-5.6.3 distro-1.8.0 dpath-2.1.6 dulwich-0.21.6 dvc-3.29.0 dvc-data-2.20.0 dvc-http-2.30.2 dvc-objects-1.2.0 dvc-render-0.6.0 dvc-studio-client-0.15.0 dvc-task-0.3.0 filelock-3.13.1 flatten-dict-0.4.2 flufl.lock-7.1.1 frozenlist-1.4.0 fsspec-2023.10.0 funcy-2.0 grandalf-0.8 gto-1.5.0 hydra-core-1.3.2 iterative-telemetry-0.0.8 kombu-5.3.3 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.0.4 networkx-3.2.1 omegaconf-2.3.0 orjson-3.9.10 pathspec-0.11.2 platformdirs-3.11.0 pycparser-2.21 pydantic-2.5.0 pydantic-core-2.14.1 pydot-1.4.2 pygit2-1.13.2 pygtrie-2.5.0 rich-13.6.0 ruamel.yaml-0.18.5 ruamel.yaml.clib-0.2.8 scmrepo-1.4.1 semver-3.0.2 shortuuid-1.0.11 shtab-1.6.4 sqltrie-0.8.0 tomlkit-0.12.3 tqdm-4.66.1 typer-0.9.0 vine-5.1.0 voluptuous-0.14.0 yarl-1.9.2 zc.lockfile-3.0.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: antlr4-python3-runtime is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisez un projet dvc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DVC repository.\n",
      "\n",
      "You can now commit the changes to git.\n",
      "\n",
      "+---------------------------------------------------------------------+\n",
      "|                                                                     |\n",
      "|        DVC has enabled anonymous aggregate usage analytics.         |\n",
      "|     Read the analytics documentation (and how to opt-out) here:     |\n",
      "|             <https://dvc.org/doc/user-guide/analytics>              |\n",
      "|                                                                     |\n",
      "+---------------------------------------------------------------------+\n",
      "\n",
      "What's next?\n",
      "------------\n",
      "- Check out the documentation: <https://dvc.org/doc>\n",
      "- Get help and share ideas: <https://dvc.org/chat>\n",
      "- Star us on GitHub: <https://github.com/iterative/dvc>\n"
     ]
    }
   ],
   "source": [
    "!dvc init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardez ce qui a été créé avec un git status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "\n",
      "No commits yet\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git rm --cached <file>...\" to unstage)\n",
      "\tnew file:   .dvc/.gitignore\n",
      "\tnew file:   .dvc/config\n",
      "\tnew file:   .dvcignore\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\tMlflow_tp_efrei_etudiant.ipynb\n",
      "\t\"TP mlflow \\303\\251tudiants_.docx\"\n",
      "\tmlartifacts/\n",
      "\tmlflow_cnn_serveur_central_etudiant.ipynb\n",
      "\tmlruns/\n",
      "\trequirements.txt\n",
      "\tvenv/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajoutez les nouveaux fichiers dans le fichier .dvc : le .gitignore, le fichier config, et le .dvcignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master (root-commit) 3b06766] initialize repo\n",
      " 3 files changed, 6 insertions(+)\n",
      " create mode 100644 .dvc/.gitignore\n",
      " create mode 100644 .dvc/config\n",
      " create mode 100644 .dvcignore\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"initialize repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons ajouter un stockage distant dans le cloud. Ici nous allons utiliser un stockage dans un dossier local pour les besoins du tp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 'dvc-remote' as a default remote.\n"
     ]
    }
   ],
   "source": [
    "!dvc remote add -d dvc-remote /tmp/dvc-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut regarder le contenu du fichier dvc config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Get' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Ecole d'inge\\Application of big data\\Mlflow_tp_efrei_etudiant.ipynb Cell 61\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Ecole%20d%27inge/Application%20of%20big%20data/Mlflow_tp_efrei_etudiant.ipynb#Y114sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#cat .dvc/config\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Ecole%20d%27inge/Application%20of%20big%20data/Mlflow_tp_efrei_etudiant.ipynb#Y114sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m Get\u001b[39m-\u001b[39mContent \u001b[39m.\u001b[39mdvc\u001b[39m/\u001b[39mconfig\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Get' is not defined"
     ]
    }
   ],
   "source": [
    "#cat .dvc/config\n",
    "Get-Content .dvc/config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons que l'url locale a bien été ajouté. Nous pouvons commiter ces changements à git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master e5664c1] add remote storage\n",
      " 1 file changed, 4 insertions(+)\n"
     ]
    }
   ],
   "source": [
    "!git commit .dvc/config -m \"add remote storage\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer un dossier data pour stocker notre dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Télécharger notre dataset et le placer dans le dossier data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset_source_url = \"https://archive.ics.uci.edu/static/public/275/bike+sharing+dataset.zip\"\n",
    "\n",
    "content = requests.get(dataset_source_url).content\n",
    "with zipfile.ZipFile(io.BytesIO(content)) as arc:\n",
    "    raw_data = pd.read_csv(arc.open(\"hour.csv\"), header=0, sep=',', parse_dates=['dteday'], index_col='dteday')\n",
    "\n",
    "raw_data.to_csv(path_or_buf=\"data/hour.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons le contenu de data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 2E08-72D9\n",
      "\n",
      " R�pertoire de c:\\Ecole d'inge\\Application of big data\\data\n",
      "\n",
      "15/11/2023  13:21    <DIR>          .\n",
      "15/11/2023  13:21    <DIR>          ..\n",
      "15/11/2023  13:21         1�161�688 hour.csv\n",
      "               1 fichier(s)        1�161�688 octets\n",
      "               2 R�p(s)  43�603�546�112 octets libres\n"
     ]
    }
   ],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous voulons commencer à suivre les changements d'un fichier il nous suffit de l'ajouter via dvc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add 'data\\.gitignore' 'data\\hour.csv.dvc'\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⠋ Checking graph\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!dvc add data/hour.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons regarder de nouveau ce qu'il y a dans notre dossier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 2E08-72D9\n",
      "\n",
      " R�pertoire de c:\\Ecole d'inge\\Application of big data\n",
      "\n",
      "\n",
      " R�pertoire de c:\\Ecole d'inge\\Application of big data\\data\n",
      "\n",
      "15/11/2023  13:21    <DIR>          .\n",
      "15/11/2023  13:21    <DIR>          ..\n",
      "15/11/2023  13:21                11 .gitignore\n",
      "15/11/2023  13:21         1�161�688 hour.csv\n",
      "15/11/2023  13:21                96 hour.csv.dvc\n",
      "               3 fichier(s)        1�161�795 octets\n",
      "               2 R�p(s)  43�602�079�744 octets libres\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fichier introuvable\n"
     ]
    }
   ],
   "source": [
    "ls -l data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons un nouveau fichier .dvc.   \n",
    "Si nous regardons à l'intérieur : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3321368116.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Get-Content data/hour.csv.dvc\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Get-Content data/hour.csv.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que le fichier contient des informations à propos de notre csv :  \n",
    "- Un hash du fichier \n",
    "- l'algorithme de hashage utilisé\n",
    "- la taille\n",
    "- le chemin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un gitignore a été créé par défaut, si on regarde à l'intérieur : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (82430977.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[22], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Get-Content data/.gitignore\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Get-Content data/.gitignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que notre csv y est renseigné."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant ajoutons le nouveau fichier data/hour.csv.dvc et le fichier data/.gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add data/.gitignore data/hour.csv.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et commitons le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 2fc0189] add .dvc file to track hours.csv file\n",
      " 2 files changed, 6 insertions(+)\n",
      " create mode 100644 data/.gitignore\n",
      " create mode 100644 data/hour.csv.dvc\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"add .dvc file to track hours.csv file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une bonne idée est de créer un tag pour chaque version de notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git tag -a 'v1' -m 'raw_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre donnée est toujours sur notre dossier en local, maintenant nous devons l'envoyer sur notre stokage distant (qui pour rappel et en fait un autre dossier local). Pour ça nous utilisons la commande dvc push."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 file pushed\n"
     ]
    }
   ],
   "source": [
    "!dvc push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut regarder dans notre \"remote storage\" ce que nous avons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Le format du param�tre est incorrect - \"mp\".\n"
     ]
    }
   ],
   "source": [
    "ls -lR /tmp/dvc-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons voir que notre fichier est présent dans le dossier mais avec un nom différent, ce nom correspond au hash de la donnée du fichier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nos données sont sauvegardées à distance, nous pouvons les supprimer localement. Sauf le fichier .dvc ! Car sinon vous perdrez le lien avec vos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 2E08-72D9\n",
      "\n",
      " R�pertoire de c:\\Ecole d'inge\\Application of big data\\data\n",
      "\n",
      "15/11/2023  13:28    <DIR>          .\n",
      "15/11/2023  13:21    <DIR>          ..\n",
      "15/11/2023  13:21                11 .gitignore\n",
      "15/11/2023  13:28                90 hour.csv.dvc\n",
      "               2 fichier(s)              101 octets\n",
      "               2 R�p(s)  43�595�780�096 octets libres\n"
     ]
    }
   ],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2059987501.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[62], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    rm -r data/hour.csv\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "rm -r data/hour.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 2E08-72D9\n",
      "\n",
      " R�pertoire de c:\\Ecole d'inge\\Application of big data\\data\n",
      "\n",
      "15/11/2023  13:25    <DIR>          .\n",
      "15/11/2023  13:21    <DIR>          ..\n",
      "15/11/2023  13:21                11 .gitignore\n",
      "15/11/2023  13:21                96 hour.csv.dvc\n",
      "               2 fichier(s)              107 octets\n",
      "               2 R�p(s)  43�602�497�536 octets libres\n"
     ]
    }
   ],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autre emplacement où vos données résident est le dossier .dvc/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
      "'Get-Content' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!ls .dvc/cache/files/md5/0a/1c63297d478edfdcc18433bb509cd5\n",
    "!Get-Content .dvc/cache/files/md5/0a/1c63297d478edfdcc18433bb509cd5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous supprimons aussi les données à l'intérieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!rm -r .dvc/cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous voulons récupérer nos données localement, nous pouvons utiliser dvc pull pour récupérer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A       data\\hour.csv\n",
      "1 file added and 1 file fetched\n"
     ]
    }
   ],
   "source": [
    "!dvc pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous regardons dans le dossier data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 2E08-72D9\n",
      "\n",
      " R�pertoire de c:\\Ecole d'inge\\Application of big data\n",
      "\n",
      "\n",
      " R�pertoire de c:\\Ecole d'inge\\Application of big data\\data\n",
      "\n",
      "15/11/2023  13:26    <DIR>          .\n",
      "15/11/2023  13:21    <DIR>          ..\n",
      "15/11/2023  13:21                11 .gitignore\n",
      "15/11/2023  13:26         1�161�688 hour.csv\n",
      "15/11/2023  13:21                96 hour.csv.dvc\n",
      "               3 fichier(s)        1�161�795 octets\n",
      "               2 R�p(s)  43�596�263�424 octets libres\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fichier introuvable\n"
     ]
    }
   ],
   "source": [
    "ls -l data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre fichier est de retour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant modifions nos données !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Option non valide - \"hour.csv\".\n"
     ]
    }
   ],
   "source": [
    "ls data/hour.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sed' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "(Get-Content -Path \"data/hour.csv\" -Raw) -replace \"(?m)^.*\\r?\\n\", \"\" | Set-Content -Path \"data/hour.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l data/hour.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et répétons les opérations précédantes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add 'data\\hour.csv.dvc'\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⠋ Checking graph\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!dvc add data/hour.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add data/hour.csv.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: pathspec 'remove' did not match any file(s) known to git\n",
      "error: pathspec '1000' did not match any file(s) known to git\n",
      "error: pathspec 'lines'' did not match any file(s) known to git\n"
     ]
    }
   ],
   "source": [
    "!git commit -m 'data: remove 1000 lines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: too many arguments\n"
     ]
    }
   ],
   "source": [
    "!git tag -a 'v2' -m 'removed 1000 lines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 file pushed\n"
     ]
    }
   ],
   "source": [
    "!dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!rm -r data/hour.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf .dvc/cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant regarder dans notre git log, et voir l'historique des modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit 2fc018930fdde1f12113fa77e531147b45eb551f\n",
      "Author: STANG94 <stephane.tang@efrei.net>\n",
      "Date:   Wed Nov 15 13:23:01 2023 +0100\n",
      "\n",
      "    add .dvc file to track hours.csv file\n",
      "\n",
      "commit e5664c16479717c59ea4484ad6e8d54d32a48ed4\n",
      "Author: STANG94 <stephane.tang@efrei.net>\n",
      "Date:   Wed Nov 15 13:21:01 2023 +0100\n",
      "\n",
      "    add remote storage\n",
      "\n",
      "commit 3b06766748f86bcc54dd6b0c9ade031020721bcf\n",
      "Author: STANG94 <stephane.tang@efrei.net>\n",
      "Date:   Wed Nov 15 13:17:43 2023 +0100\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    initialize repo\n"
     ]
    }
   ],
   "source": [
    "!git log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour accéder et extraire des versions spécifiques de nos données nous pouvons utiliser le package dvc en python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Select-String' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOutput\\ndvc==3.29.0\\ndvc-data==2.20.0\\ndvc-http==2.30.2\\ndvc-objects==1.2.0\\ndvc-render==0.6.0\\ndvc-studio-client==0.15.0\\ndvc-task==0.3.0\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip freeze | Select-String -Pattern \"dvc\"\n",
    "\n",
    "\"\"\"\n",
    "Output\n",
    "dvc==3.29.0\n",
    "dvc-data==2.20.0\n",
    "dvc-http==2.30.2\n",
    "dvc-objects==1.2.0\n",
    "dvc-render==0.6.0\n",
    "dvc-studio-client==0.15.0\n",
    "dvc-task==0.3.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dvc\n",
    "import dvc.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "CloneError",
     "evalue": "SCM error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotGitRepository\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\scmrepo\\git\\backend\\dulwich\\__init__.py:227\u001b[0m, in \u001b[0;36mDulwichBackend.clone\u001b[1;34m(cls, url, to_path, shallow_branch, progress, bare, mirror)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     repo \u001b[39m=\u001b[39m clone_from()\n\u001b[0;32m    229\u001b[0m \u001b[39mwith\u001b[39;00m closing(repo):\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\dulwich\\porcelain.py:542\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(source, target, bare, checkout, errstream, outstream, origin, depth, branch, config, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m (client, path) \u001b[39m=\u001b[39m get_transport_and_path(\n\u001b[0;32m    540\u001b[0m     source, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 542\u001b[0m \u001b[39mreturn\u001b[39;00m client\u001b[39m.\u001b[39;49mclone(\n\u001b[0;32m    543\u001b[0m     path,\n\u001b[0;32m    544\u001b[0m     target,\n\u001b[0;32m    545\u001b[0m     mkdir\u001b[39m=\u001b[39;49mmkdir,\n\u001b[0;32m    546\u001b[0m     bare\u001b[39m=\u001b[39;49mbare,\n\u001b[0;32m    547\u001b[0m     origin\u001b[39m=\u001b[39;49morigin,\n\u001b[0;32m    548\u001b[0m     checkout\u001b[39m=\u001b[39;49mcheckout,\n\u001b[0;32m    549\u001b[0m     branch\u001b[39m=\u001b[39;49mbranch,\n\u001b[0;32m    550\u001b[0m     progress\u001b[39m=\u001b[39;49merrstream\u001b[39m.\u001b[39;49mwrite,\n\u001b[0;32m    551\u001b[0m     depth\u001b[39m=\u001b[39;49mdepth,\n\u001b[0;32m    552\u001b[0m )\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\dulwich\\client.py:738\u001b[0m, in \u001b[0;36mGitClient.clone\u001b[1;34m(self, path, target_path, mkdir, bare, origin, checkout, branch, progress, depth)\u001b[0m\n\u001b[0;32m    737\u001b[0m ref_message \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclone: from \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m encoded_path\n\u001b[1;32m--> 738\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetch(path, target, progress\u001b[39m=\u001b[39;49mprogress, depth\u001b[39m=\u001b[39;49mdepth)\n\u001b[0;32m    739\u001b[0m _import_remote_refs(\n\u001b[0;32m    740\u001b[0m     target\u001b[39m.\u001b[39mrefs, origin, result\u001b[39m.\u001b[39mrefs, message\u001b[39m=\u001b[39mref_message)\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\dulwich\\client.py:1484\u001b[0m, in \u001b[0;36mLocalGitClient.fetch\u001b[1;34m(self, path, target, determine_wants, progress, depth)\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fetch into a target repository.\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m \n\u001b[0;32m   1471\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1482\u001b[0m \n\u001b[0;32m   1483\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1484\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open_repo(path) \u001b[39mas\u001b[39;00m r:\n\u001b[0;32m   1485\u001b[0m     refs \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mfetch(\n\u001b[0;32m   1486\u001b[0m         target,\n\u001b[0;32m   1487\u001b[0m         determine_wants\u001b[39m=\u001b[39mdetermine_wants,\n\u001b[0;32m   1488\u001b[0m         progress\u001b[39m=\u001b[39mprogress,\n\u001b[0;32m   1489\u001b[0m         depth\u001b[39m=\u001b[39mdepth,\n\u001b[0;32m   1490\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\dulwich\\client.py:1406\u001b[0m, in \u001b[0;36mLocalGitClient._open_repo\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfsdecode(path)\n\u001b[1;32m-> 1406\u001b[0m \u001b[39mreturn\u001b[39;00m closing(Repo(path))\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\dulwich\\repo.py:1141\u001b[0m, in \u001b[0;36mRepo.__init__\u001b[1;34m(self, root, object_store, bare)\u001b[0m\n\u001b[0;32m   1140\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1141\u001b[0m         \u001b[39mraise\u001b[39;00m NotGitRepository(\n\u001b[0;32m   1142\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mNo git repository was found at \u001b[39m\u001b[39m{path}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(path\u001b[39m=\u001b[39mroot))\n\u001b[0;32m   1143\u001b[0m         )\n\u001b[0;32m   1145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbare \u001b[39m=\u001b[39m bare\n",
      "\u001b[1;31mNotGitRepository\u001b[0m: No git repository was found at dvc-remote",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mCloneError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\dvc\\scm.py:158\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(url, to_path, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     git \u001b[39m=\u001b[39m Git\u001b[39m.\u001b[39mclone(url, to_path, progress\u001b[39m=\u001b[39mpbar\u001b[39m.\u001b[39mupdate_git, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    159\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mshallow_branch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\scmrepo\\git\\__init__.py:148\u001b[0m, in \u001b[0;36mGit.clone\u001b[1;34m(cls, url, to_path, rev, bare, mirror, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m     backend\u001b[39m.\u001b[39mclone(url, to_path, bare\u001b[39m=\u001b[39mbare, mirror\u001b[39m=\u001b[39mmirror, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    149\u001b[0m     repo \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(to_path)\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\scmrepo\\git\\backend\\dulwich\\__init__.py:235\u001b[0m, in \u001b[0;36mDulwichBackend.clone\u001b[1;34m(cls, url, to_path, shallow_branch, progress, bare, mirror)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m--> 235\u001b[0m     \u001b[39mraise\u001b[39;00m CloneError(url, to_path) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[1;31mCloneError\u001b[0m: Failed to clone repo 'dvc-remote' to 'C:\\Users\\tangs\\AppData\\Local\\Temp\\tmp6w4608tidvc-clone'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mCloneError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Ecole d'inge\\Application of big data\\Mlflow_tp_efrei_etudiant.ipynb Cell 121\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Ecole%20d%27inge/Application%20of%20big%20data/Mlflow_tp_efrei_etudiant.ipynb#Y230sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m2fc018930fdde1f12113fa77e531147b45eb551f\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Ecole%20d%27inge/Application%20of%20big%20data/Mlflow_tp_efrei_etudiant.ipynb#Y230sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#revision can be git commit id, commit tag, ...\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Ecole%20d%27inge/Application%20of%20big%20data/Mlflow_tp_efrei_etudiant.ipynb#Y230sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m data_url \u001b[39m=\u001b[39m dvc\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mget_url(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Ecole%20d%27inge/Application%20of%20big%20data/Mlflow_tp_efrei_etudiant.ipynb#Y230sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     path\u001b[39m=\u001b[39;49mpath, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Ecole%20d%27inge/Application%20of%20big%20data/Mlflow_tp_efrei_etudiant.ipynb#Y230sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     repo\u001b[39m=\u001b[39;49mrepo,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Ecole%20d%27inge/Application%20of%20big%20data/Mlflow_tp_efrei_etudiant.ipynb#Y230sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     rev\u001b[39m=\u001b[39;49mversion\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Ecole%20d%27inge/Application%20of%20big%20data/Mlflow_tp_efrei_etudiant.ipynb#Y230sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\dvc\\api\\data.py:45\u001b[0m, in \u001b[0;36mget_url\u001b[1;34m(path, repo, rev, remote)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m remote:\n\u001b[0;32m     44\u001b[0m     repo_kwargs[\u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mcore\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mremote\u001b[39m\u001b[39m\"\u001b[39m: remote}}\n\u001b[1;32m---> 45\u001b[0m \u001b[39mwith\u001b[39;00m Repo\u001b[39m.\u001b[39mopen(\n\u001b[0;32m     46\u001b[0m     repo, rev\u001b[39m=\u001b[39mrev, subrepos\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, uninitialized\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrepo_kwargs\n\u001b[0;32m     47\u001b[0m ) \u001b[39mas\u001b[39;00m _repo:\n\u001b[0;32m     48\u001b[0m     index, entry \u001b[39m=\u001b[39m _repo\u001b[39m.\u001b[39mget_data_index_entry(path)\n\u001b[0;32m     49\u001b[0m     \u001b[39mwith\u001b[39;00m reraise(\n\u001b[0;32m     50\u001b[0m         (StorageKeyError, \u001b[39mValueError\u001b[39;00m),\n\u001b[0;32m     51\u001b[0m         NoRemoteError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno remote specified in \u001b[39m\u001b[39m{\u001b[39;00m_repo\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     52\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\dvc\\repo\\__init__.py:304\u001b[0m, in \u001b[0;36mRepo.open\u001b[1;34m(url, *args, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(url: Optional[\u001b[39mstr\u001b[39m], \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRepo\u001b[39m\u001b[39m\"\u001b[39m:  \u001b[39m# noqa: A003\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mopen_repo\u001b[39;00m \u001b[39mimport\u001b[39;00m open_repo\n\u001b[1;32m--> 304\u001b[0m     \u001b[39mreturn\u001b[39;00m open_repo(url, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\dvc\\repo\\open_repo.py:64\u001b[0m, in \u001b[0;36mopen_repo\u001b[1;34m(url, *args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[39mexcept\u001b[39;00m NotDvcRepoError:\n\u001b[0;32m     62\u001b[0m         \u001b[39mpass\u001b[39;00m  \u001b[39m# fallthrough to _external_repo\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[39mreturn\u001b[39;00m _external_repo(url, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Ecole9\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\dvc\\repo\\open_repo.py:27\u001b[0m, in \u001b[0;36m_external_repo\u001b[1;34m(url, rev, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m@map_scm_exception\u001b[39m()\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_external_repo\u001b[39m(\n\u001b[0;32m     22\u001b[0m     url,\n\u001b[0;32m     23\u001b[0m     rev: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     25\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRepo\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     26\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreating external repo \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m@\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, rev)\n\u001b[1;32m---> 27\u001b[0m     path \u001b[39m=\u001b[39m _cached_clone(url, rev)\n\u001b[0;32m     28\u001b[0m     \u001b[39m# Local HEAD points to the tip of whatever branch we first cloned from\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[39m# (which may not be the default branch), use origin/HEAD here to get\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[39m# the tip of the default branch\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     rev \u001b[39m=\u001b[39m rev \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrefs/remotes/origin/HEAD\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\dvc\\repo\\open_repo.py:138\u001b[0m, in \u001b[0;36m_cached_clone\u001b[1;34m(url, rev)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mshutil\u001b[39;00m \u001b[39mimport\u001b[39;00m copytree\n\u001b[0;32m    136\u001b[0m \u001b[39m# even if we have already cloned this repo, we may need to\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m# fetch/fast-forward to get specified rev\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m clone_path, shallow \u001b[39m=\u001b[39m _clone_default_branch(url, rev)\n\u001b[0;32m    140\u001b[0m \u001b[39mif\u001b[39;00m url \u001b[39min\u001b[39;00m CLONES:\n\u001b[0;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m CLONES[url][\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\funcy\\decorators.py:47\u001b[0m, in \u001b[0;36mmake_decorator.<locals>._decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     46\u001b[0m     call \u001b[39m=\u001b[39m Call(func, args, kwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m deco(call, \u001b[39m*\u001b[39mdargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdkwargs)\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\funcy\\flow.py:246\u001b[0m, in \u001b[0;36mwrap_with\u001b[1;34m(call, ctx)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Turn context manager into a decorator\"\"\"\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[39mwith\u001b[39;00m ctx:\n\u001b[1;32m--> 246\u001b[0m     \u001b[39mreturn\u001b[39;00m call()\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\funcy\\decorators.py:68\u001b[0m, in \u001b[0;36mCall.__call__\u001b[1;34m(self, *a, **kw)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m a \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw:\n\u001b[1;32m---> 68\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kwargs)\n\u001b[0;32m     69\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func(\u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_args \u001b[39m+\u001b[39m a), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw))\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\dvc\\repo\\open_repo.py:202\u001b[0m, in \u001b[0;36m_clone_default_branch\u001b[1;34m(url, rev)\u001b[0m\n\u001b[0;32m    200\u001b[0m             _remove(git_dir)\n\u001b[0;32m    201\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m git:\n\u001b[1;32m--> 202\u001b[0m     git \u001b[39m=\u001b[39m clone(url, clone_path)\n\u001b[0;32m    203\u001b[0m     shallow \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    204\u001b[0m CLONES[url] \u001b[39m=\u001b[39m (clone_path, shallow)\n",
      "File \u001b[1;32mc:\\Ecole d'inge\\Application of big data\\venv\\lib\\site-packages\\dvc\\scm.py:163\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(url, to_path, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m git\n\u001b[0;32m    162\u001b[0m \u001b[39mexcept\u001b[39;00m InternalCloneError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m--> 163\u001b[0m     \u001b[39mraise\u001b[39;00m CloneError(\u001b[39m\"\u001b[39m\u001b[39mSCM error\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[1;31mCloneError\u001b[0m: SCM error"
     ]
    }
   ],
   "source": [
    "path=\"data/hour.csv.dvc\"\n",
    "repo=\"dvc-remote\"\n",
    "version=\"2fc018930fdde1f12113fa77e531147b45eb551f\"\n",
    "\n",
    "#revision can be git commit id, commit tag, ...\n",
    "\n",
    "data_url = dvc.api.get_url(\n",
    "    path=path, \n",
    "    repo=repo,\n",
    "    rev=version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_URI = mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "print(MLFLOW_TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'389645318088192245'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "configure_experiment(name=\"ML_EXP_WITH_DVC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dteday  instant  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
      "0  2011-01-01        1       1   0     1   0        0        6           0   \n",
      "1  2011-01-01        2       1   0     1   1        0        6           0   \n",
      "2  2011-01-01        3       1   0     1   2        0        6           0   \n",
      "3  2011-01-01        4       1   0     1   3        0        6           0   \n",
      "4  2011-01-01        5       1   0     1   4        0        6           0   \n",
      "\n",
      "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
      "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
      "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
      "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
      "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
      "4           1  0.24  0.2879  0.75        0.0       0           1    1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_url, sep=\",\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, time\n",
    "\n",
    "data.index = raw_data.apply(\n",
    "    lambda row: datetime.combine(row.name, time(hour=int(row['hr']))), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dteday</th>\n",
       "      <th>instant</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-01 00:00:00</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 01:00:00</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 02:00:00</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 03:00:00</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 04:00:00</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         dteday  instant  season  yr  mnth  hr  holiday  \\\n",
       "2011-01-01 00:00:00  2011-01-01        1       1   0     1   0        0   \n",
       "2011-01-01 01:00:00  2011-01-01        2       1   0     1   1        0   \n",
       "2011-01-01 02:00:00  2011-01-01        3       1   0     1   2        0   \n",
       "2011-01-01 03:00:00  2011-01-01        4       1   0     1   3        0   \n",
       "2011-01-01 04:00:00  2011-01-01        5       1   0     1   4        0   \n",
       "\n",
       "                     weekday  workingday  weathersit  temp   atemp   hum  \\\n",
       "2011-01-01 00:00:00        6           0           1  0.24  0.2879  0.81   \n",
       "2011-01-01 01:00:00        6           0           1  0.22  0.2727  0.80   \n",
       "2011-01-01 02:00:00        6           0           1  0.22  0.2727  0.80   \n",
       "2011-01-01 03:00:00        6           0           1  0.24  0.2879  0.75   \n",
       "2011-01-01 04:00:00        6           0           1  0.24  0.2879  0.75   \n",
       "\n",
       "                     windspeed  casual  registered  cnt  \n",
       "2011-01-01 00:00:00        0.0       3          13   16  \n",
       "2011-01-01 01:00:00        0.0       8          32   40  \n",
       "2011-01-01 02:00:00        0.0       5          27   32  \n",
       "2011-01-01 03:00:00        0.0       3          10   13  \n",
       "2011-01-01 04:00:00        0.0       0           1    1  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant utiliser 2 mécanismes pour ajouter plus d'informations sur notre jeu de données dans MLFlow :    \n",
    "-Grâce à dvc, nous avons maintenant des liens vers les différentes versions de notre jeu de données.   \n",
    "-Nous pouvons l'utiliser en combinaison avec le module mlflow.data pour ajouter plus d'informations sur notre jeu de données.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'cnt'\n",
    "prediction = 'prediction'\n",
    "numerical_features = ['temp', 'atemp', 'hum', 'windspeed', 'hr', 'weekday']\n",
    "categorical_features = ['season', 'holiday', 'workingday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2011-01-01 00:00:00'\n",
    "end_date = '2011-01-28 23:00:00'\n",
    "dataset = data.loc[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         dteday  instant  season  yr  mnth  hr  holiday  \\\n",
      "2011-01-01 00:00:00  2011-01-01        1       1   0     1   0        0   \n",
      "2011-01-01 01:00:00  2011-01-01        2       1   0     1   1        0   \n",
      "2011-01-01 02:00:00  2011-01-01        3       1   0     1   2        0   \n",
      "2011-01-01 03:00:00  2011-01-01        4       1   0     1   3        0   \n",
      "2011-01-01 04:00:00  2011-01-01        5       1   0     1   4        0   \n",
      "...                         ...      ...     ...  ..   ...  ..      ...   \n",
      "2011-01-28 19:00:00  2011-01-28      614       1   0     1  19        0   \n",
      "2011-01-28 20:00:00  2011-01-28      615       1   0     1  20        0   \n",
      "2011-01-28 21:00:00  2011-01-28      616       1   0     1  21        0   \n",
      "2011-01-28 22:00:00  2011-01-28      617       1   0     1  22        0   \n",
      "2011-01-28 23:00:00  2011-01-28      618       1   0     1  23        0   \n",
      "\n",
      "                     weekday  workingday  weathersit  temp   atemp   hum  \\\n",
      "2011-01-01 00:00:00        6           0           1  0.24  0.2879  0.81   \n",
      "2011-01-01 01:00:00        6           0           1  0.22  0.2727  0.80   \n",
      "2011-01-01 02:00:00        6           0           1  0.22  0.2727  0.80   \n",
      "2011-01-01 03:00:00        6           0           1  0.24  0.2879  0.75   \n",
      "2011-01-01 04:00:00        6           0           1  0.24  0.2879  0.75   \n",
      "...                      ...         ...         ...   ...     ...   ...   \n",
      "2011-01-28 19:00:00        5           1           2  0.24  0.2424  0.75   \n",
      "2011-01-28 20:00:00        5           1           2  0.24  0.2273  0.70   \n",
      "2011-01-28 21:00:00        5           1           2  0.22  0.2273  0.75   \n",
      "2011-01-28 22:00:00        5           1           1  0.24  0.2121  0.65   \n",
      "2011-01-28 23:00:00        5           1           1  0.24  0.2273  0.60   \n",
      "\n",
      "                     windspeed  casual  registered  cnt  \n",
      "2011-01-01 00:00:00     0.0000       3          13   16  \n",
      "2011-01-01 01:00:00     0.0000       8          32   40  \n",
      "2011-01-01 02:00:00     0.0000       5          27   32  \n",
      "2011-01-01 03:00:00     0.0000       3          10   13  \n",
      "2011-01-01 04:00:00     0.0000       0           1    1  \n",
      "...                        ...     ...         ...  ...  \n",
      "2011-01-28 19:00:00     0.1343       5          84   89  \n",
      "2011-01-28 20:00:00     0.1940       1          61   62  \n",
      "2011-01-28 21:00:00     0.1343       1          57   58  \n",
      "2011-01-28 22:00:00     0.3582       0          26   26  \n",
      "2011-01-28 23:00:00     0.2239       1          22   23  \n",
      "\n",
      "[618 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la même cellule : \n",
    "\n",
    "1)\n",
    "- Aller voir la documentation du module mlflow data et importer le bon objet pour les données pandas.  \n",
    "- Créer un run avec la date, l'heure, etc. comme nom.    \n",
    "- pour l'entrainement vous pouvez utiliser le chemin vers votre dataset lors de la création de votre dataframe.   \n",
    "\n",
    "2) \n",
    "- logger le dataset avec la méthode appropriée.  \n",
    "- logger également le chemin vers votre dataset.   \n",
    "- logger la version du dataset utilisée.   \n",
    "\n",
    "3)\n",
    "- Créer un fichier texte et logger le en tant qu'artifact. Dans ce fichier vous pourrez indiquer la colonne qui a servi de target, les features numériques et les features catégorielles.\n",
    "\n",
    "4)\n",
    "- N'oubliez pas d'utiliser la fonction mlflow.end_run() si vous n'avez pas utilisez de with pour le run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Déploiement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Trouver comment transitionner un modèle en état staging et ensuite dans l'état production. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Créer une fonction qui récupère la version du modèle avec les meilleurs metrics d'entraînement et qui transitionne ce modèle dans l'état production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Le serveur mlflow peut vous fournir des prédictions à partir des modèles enregistrés. Faites en sorte d'obtenir une prédiction de votre dernier modèle en requêtant le serveur mlflow. (Voir : https://mlflow.org/docs/latest/models.html#command-line-interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.Créer un script à part qui pull le dernier modèle depuis le model registry. Plus tard vous pourrez utiliser ce script pour récupérer le modèle dans une api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Entraînement d'un CNN et log des metrics dans MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir le notebook donnée par le formateur."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow_tp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
